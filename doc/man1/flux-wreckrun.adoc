// flux-help-include: yes
FLUX-WRECKRUN(1)
================
:doctype: manpage


NAME
----
flux-wreckrun - Flux utility for remote execution


SYNOPSIS
--------
[verse]
'flux wreckrun' [-n <ntasks>] [-N <nnodes>] [-t <tasks-per-node>]
                [-l|--label-io] [-d|--detach] [-o|--options='OPTIONS']
                [-O|--output='FILENAME'] [-E|--error='FILENAME']
                [-i|--input='HOW'] [-I, --immediate] ['COMMANDS'...]


DESCRIPTION
-----------

'flux wreckrun' is the front-end command for launching tasks within
a Flux comms session using the WRECK remote execution prototype.

WRECK (Worthless Remote Execution using CMB KVS) is a prototype
implementation of remote execution for Flux which demonstrates
the usage of the Flux key-value store as the vehicle for transmitting
and storing information about lightweight jobs (LWJs). The basic
mode of operation is described in xref:wreck-operation[].

OPTIONS
-------

include::wreck-options.adoc[]

--detach::
-d::
	Detach immediately after issuing a run request and print new jobid
	to stdout.

--wait-until='state'::
-w 'state'::
	Ignore all stdio, and instead just wait until the job reaches
        a state of 'state' (i.e. "starting", "running", or "complete") and
        exit immediately.

--immediate::
-I::
       Bypass scheduler and run job immediately.

--jobid='id'::
-J 'id' ::
	Run a job immediately across the same ranks as a previous job. By
	default, the number of tasks is set to the number of ranks such
	that one task per broker rank is executed.

include::wreck-extra-options.adoc[]

OPERATION
----------
[[wreck-operation]]

Briefly, the method of operation for remote execution via WRECK
consists of the following steps:

. The front-end command ('wreckrun') allocates a new job directory
  entry in the kvs under the 'lwj' key. This operation is performed as
  a 'job.create' request, which results in a new directory in the kvs,
  such as 'lwj.<jobid>'.  The full directory path is dependent on
  settings but can be determined via "flux wreck last-jobid -p".
  Initially, a job state is 'reserved' and stored in the 'state'
  directory, such as 'lwj.<jobid>.state'.
. The front-end command now fills in requisite and optional job
  information under the 'lwj' jobid directory such as the command-line
  to run, how many tasks to run on each rank, the job environment,
  etc.
. The front-end command then issues a 'wrexec.run' event, which
  is handled by the 'wrexec' module. The 'wrexec' module spawns
  a daemon to handle the request for the jobid in question. The
  'wrexecd' daemon pulls information for the job in question
  directly from the local kvs cache and spawns required tasks.
. Once the first task is started by rank 0, the job state is updated
  to 'starting'.
. After starting tasks, all 'wrexecd' daemons synchronize on a
  barrier, and the job state is updated to 'running'.
. As the tasks run, stdio is either stored in the KVS or sent directly
  to a per-job service as messages (when '-o nokz' job option is set),
  and output is optionally displayed to user or redirected as requested
  by command line options.
. As tasks exit, their exit status is recorded in the kvs.
. After all tasks have exited, 'wrexecd' daemons synchronize again
  and rank 0 updates the job state to 'completed'.

This simple scheme offers a scalable and semi-flexible method
for launching tasks within a Flux comms session. In conjunction
with the Flux PMI library, even MPI applications may be launched
at scale via 'flux wreckrun'. See xref:wreckrun-examples[] for
examples.

OUTPUT ENVIRONMENT VARIABLES
----------------------------
[[wreckrun-output-environment-variables]]

The following environment variables will be set in the environment of
tasks executed via 'flux wreckrun'.

*FLUX_JOB_ID*::
The job ID of the current job

*FLUX_JOB_NNODES*::
Number of nodes used in the current job

*FLUX_NODE_ID*::
The relative node ID of the current node

*FLUX_JOB_SIZE*::
The total number of tasks to be run in the current job  

*FLUX_LOCAL_RANKS*::
Comma separated list of task ranks run on the current node

*FLUX_TASK_RANK*::
The relative task rank of the current task

*FLUX_TASK_LOCAL_ID*::
The relative task rank of the current task on the current node


EXAMPLES
--------
[[wreckrun-examples]]

Once a Flux comms session is running, ensure the required modules
for 'wreck' are loaded:

------------------------------
$ flux module list | egrep 'resource|job|wrexec'
resource-hwloc        44673 498403A idle 0
wrexec                42052 2CA4CAT idle 0
job                   28959 60947AA idle 0
$
------------------------------

To ensure wrexec is working properly, run hostname across
4 nodes of the Flux comms session:

------------------------------
$ flux wreckrun -l -N4 hostname
wreckrun: 0.000s: Sending LWJ request for 1 tasks (cmdline "hostname")
wreckrun: 0.007s: Registered jobid 1
wreckrun: Allocating 4 tasks across 4 available nodes..
wreckrun: tasks per node: node[0-3]: 1
wreckrun: 0.009s: Sending run event
wreckrun: 0.012s: State = reserved
wreckrun: 0.020s: State = starting
wreckrun: 0.082s: State = running
0: hype349
2: hype351
3: hype352
1: hype350
wreckrun: 0.140s: State = complete
wreckrun: All tasks completed successfully.
------------------------------

'flux wreckrun' uses the Flux kvs for most of its functionality,
and thus information about lightweight jobs (LWJs) that are
executed will be stored in the 'lwj.<jobid>' directory in the kvs.
To see what kind of information is stored there, use the
'flux kvs' utility:

------------------------------
$ flux kvs dir lwj.1
------------------------------

.Running an MPI application with 'flux wreckrun'

Launching an MPI application with 'flux wreckrun' requires that your
version of MPI supports PMI. In this example, MVAPICH2 is used:

------------------------------
$ mpicc -v 2>&1 | head -1
mpicc for MVAPICH2 version 1.7
$ mpicc -o mpi_hello mpi_hello.c
------------------------------

In order to run using the Flux PMI library, 'LD_LIBRARY_PATH' may need
to be adjusted at runtime. For example, to use Flux's libpmi.so 
from the build directory:

------------------------------
$ export LD_LIBRARY_PATH=${flux_builddir}/src/lib/libpmi/.libs${LD_LIBRARY_PATH}
------------------------------

where '${flux_builddir}' is the path to the root of the flux-core
build directory.

Now the MPI application should run via 'flux wreckrun':

------------------------------
$ flux wreckrun -N4 ./mpi_hello
wreckrun: 0.000s: Sending LWJ request for 1 tasks (cmdline "./mpi_hello")      
wreckrun: 0.007s: Registered jobid 5                                           
wreckrun: Allocating 4 tasks across 4 available nodes..                        
wreckrun: tasks per node: node[0-3]: 1                                         
wreckrun: 0.009s: Sending run event                                            
wreckrun: 0.011s: State = reserved                                             
wreckrun: 0.018s: State = starting                                             
wreckrun: 0.028s: State = running                                              
0: completed first barrier                                                     
0: completed MPI_Init in 0.039s.  There are 4 tasks                            
0: completed MPI_Finalize                                                      
wreckrun: 2.182s: State = complete                                             
wreckrun: All tasks completed successfully.                                    
------------------------------

The use of Flux PMI can be verified by checking the KVS 'pmi' directory
for the LWJ in question:

------------------------------
$ flux kvs dir pmi.5                                                         
pmi.5.hostname[2] = "-1463773063"                                              
pmi.5.hostname[0] = "-1463773575"                                              
pmi.5.hostname[1] = "-1463773319"                                              
pmi.5.hostname[3] = "-1463772807"                                              
pmi.5.uuid_2_5 = "dummy-entry"                                                 
pmi.5.uuid_0_5 = "8hiWSLCPRzqFYHNubktbCQ=="                                    
pmi.5.uuid_1_5 = "dummy-entry"                                                 
pmi.5.uuid_3_5 = "dummy-entry"                                                 
pmi.5.pmi_epidkey_2 = "1901059"                                                
pmi.5.pmi_epidkey_0 = "7209475"                                                
pmi.5.pmi_epidkey_1 = "6816259"                                                
pmi.5.pmi_epidkey_3 = "6750723"             
------------------------------


AUTHOR
------
This page is maintained by the Flux community.


RESOURCES
---------
Github: <http://github.com/flux-framework>


COPYRIGHT
---------
include::COPYRIGHT.adoc[]


SEE ALSO
--------
flux-wreck(1)
